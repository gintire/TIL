# Backup and Restore
백업과 복구 메서드에 대해서 알아봄

쿠버네티스 클러스터에서 무엇을 백업해야할지에 대해서 알아본다.

## Backup Candidates
이번 장에서 쿠버네티스 클러스터에 pod와 서비스 definition file을 통해 여러개의 어플리케이션이 배포되어있다.

ETCD 클러스터는 모든 클러스터에 대한 정보가 저장되어있는 것을 알것이다. 애플리케이션이 영구 저장소로 구성된 경우 클러스터에서 만든 리소스와 관련하여 백업을 위한 다른 후보 저장소이다.

### Imperative
```
$ kubectl create namespace new-namespace
$ kubectl create secret
$ kubectl create configmap
```
과 같이 명령어를 통해 오브젝트를 생성가능하다.

### Declarative
definition file을 생성하고 kubectl을 통해서 동작시킨다.
```
$ kubectl apply -f pod-definition.yaml
```

지금부터 관리의 편의성과 재사용성, 공유의 편의성을 위하여 하나의 폴더에 하나의 definition file을 이용하여 하나의 어플리케이션 설정을 할 것이다.

definition 파일을 github등에 백업함으로서 쉽게 설정 파일들을 보존할 수 있다.  
만약 클러스터에 문제가 생기더라도 definition파일들을 통해 쉽게 복구가능하다.

### Imperative backup
만약에 문서를 남겨두지 않은채로 imperative로 오브젝트들을 생성하였다면 어떻게 할까?

리소스의 설정값을 백업하는 좋은 방법은 kube-apiserver에 쿼리를 보내는 것이다.

Kube-apiserver로 쿼리를 하는것은 kubectl을 사용 하는 것 또는 api서버에 직접적으로 접근하여 복제본으로서 저장된 클러스터에 생성된 모든 오프젝트에 대한 리소스 설정값을 저장하는 방법이 있다.

예제>  
kubectl을 이용하여 모든 네임스페이스에 있는 pod, deployment, service를 백업하는 스크립트를 동작시켜 yaml 파일 형식으로 추출하여 파일로 저장한다.
```
kubectl get all --all-namespaces -o yaml > all-deploy-services.yaml
```
이 경우 몇몇의 리소스 그룹만 포함된다. 다른 리소스 그룹도 고려해야 한다.

HaptIO에서 만들어진 Velero라고 불리는 ARGE같은 툴을 사용하면된다.

쿠버네티스 API 통신하면서 백업을 도와준다.

### Backup - ETCD
ETCD 클러스터는 클러스터의 상태에 대한 정보를 저장한다.

클러스터 자체의 정보 (클러스터에 생성된 모든 노드와 리소스 등)는 ETCD에 저장된다.

이전에 처럼 리소스를 백업하는 대신에 서버 자체를 백업한다. 

보이는 것과 같이 ETCD 클러스터가 마스터 노드에 호스트되어 있다. 

ETCD 설정이 정의 되면 > etcd.service 에서 --date-dir에 데이터를 저장할 위치가 있다.

data-dir는 설정을 백업하거나 ETCD를 백업하는 디렉터리가 된다.

그리고 ETCD는 내장 snapshot 솔루션이 있다.

ETCD control을 이용하여 데이터베이스의 스냅샷을 생성한다.
```
$ETCDCTL_API=3 etcdctl \
    snapshot save snapshot.db
```
```
$ls
snapshot.db
```
snapshot.db 라는 snapshot이 현재 디렉터리에 생성된다. 만약 다른 디렉터리에 저장하고자 한다면 전체 경로를 적어주면된다.

status command로 스냅샷 상태를 확인할 수있다.
```
ETCDCTL_API=3 etcdctl \
    snapshot status snapshot.db
```

### Restore - ETCD
만약 복구를 하고자한다면, 다음과 같이한다.
먼저, kubeapi server service를 먼저 중단시킨다.
```
service kube-apiserver stop
```
복구 프로세스는 ETCD 클러스터를 재시작 시킬 필요가 있다.

그 후, 아까 생성했던 backup file를 사용하여 복구를 진행한다.
```
ETCDCTL_API=3 etcdctl \
  snapshot restore snapshot.db \
  --data-dir /var/lib/etcd-from-backup
```
ETCD가 백업에서 복구할 때 보관되는 스냅샷은 새 클러스터 구성을 초기화하고 ETCD의 구성원은 새 클러스터의 새 구성원으로 구성된다.

이는 새로운 멤버가 이미 존재하는 클러스터에 의도치 않게 추가되는 것을 방지한다.

--data-dir 설정은 새로운 데이터 저장 디렉터리를 지정한다.

그 후 service daemon을 다시 실행한다.

```
systemctl daemon-reload
```

마지막으로 kubeapi서버를 시작한다.
```
service kube-apiserver start
```

기억할 점,

모든 ETCDCTL 명령은 authentication을 위하여 cert 파일을 적어줘야한다.
* endpoints (end point to the outside cluster)
* cacert (cacert)
* cert (server cert)
* key (key)


## Backup Cadidates
2가지의 방법이 있다.
* ETCD 백업
* Kube-api server에 쿼리

두가지에는 각자 장단점이 있다.

만약 운영 환경에서 클러스터 외부에서의 접근이 없다면 kube-apiserver에 쿼리를 날리는 방법이 더 나은 방법이다.


### 문제
1. What is the version of ETCD running on the cluster?
```
$ kubectl logs etcd-controlplane -n kube-system
$ kubectl describe pod etcd-controlplane -n kube-system
```
2. At what address do you reach the ETCD cluster from your master/controlplane node?
```
$ kubectl describe pod etcd-controlplane -n kube-system
###  --listen-client-urls 확인
``` 

   
3. The master nodes in our cluster are planned for a regular maintenance reboot tonight. While we do not anticipate anything to go wrong, we are required to take the necessary backups. Take a snapshot of the ETCD database using the built-in snapshot functionality.
```
ETCDCTL_API=3 etcdctl \
--endpoints=https://[127.0.0.1]:2379 \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
snapshot save /opt/snapshot-pre-boot.db
```

4. 복구 한뒤 /etc/kubernets/manifests/etcd.yaml 수정
Update --data-dir to use new target location
[정답 ~~~~팁](https://github.com/mmumshad/kubernetes-the-hard-way/blob/master/practice-questions-answers/cluster-maintenance/backup-etcd/etcd-backup-and-restore.md)


--data-dir=/var/lib/etcd-from-backup
Update ETCD POD to use the new hostPath directory /var/lib/etcd-from-backup by modifying the pod definition file at /etc/kubernetes/manifests/etcd.yaml. When this file is updated, the ETCD pod is automatically re-created as this is a static pod placed under the /etc/kubernetes/manifests directory.

Update volumes and volume mounts to point to new path
```
  volumes:
  - hostPath:
      path: /var/lib/etcd-from-backup
      type: DirectoryOrCreate
    name: etcd-data
  - hostPath:
      path: /etc/kubernetes/pki/etcd
      type: DirectoryOrCreate
    name: etcd-certs
```
Note: as the ETCD pod has changed it will automatically restart, and also kube-controller-manager and kube-scheduler. Wait 1-2 to mins for this pods to restart. You can make a **watch "docker ps | grep etcd"** to see when the ETCD pod is restarted.
ㅂ
Note2: If the etcd pod is not getting Ready 1/1, then restart it by kubectl delete pod -n kube-system etcd-controlplane and wait 1 minute.
